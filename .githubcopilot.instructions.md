AIパートナー開発プロジェクト設計書 (Project: Master's Companion)

1. プロジェクトコンセプト

本プロジェクトは、単なるテキストチャットボットを超え、「肉体（3Dモデル）」と「声（合成音声）」、そして「感情と記憶」を備えたAIパートナーの構築を目的とする。

1.1 ビジョン

親密な対話体験: ユーザーを「マスター」と呼び、親愛度や文脈に応じた独自のインタラクションを提供する。

マルチモーダルな反応: テキストの内容に応じて、3Dモデルの表情（BlendShape）や声色（TTSパラメータ）が動的に変化する。

自律的な存在感: ユーザーの入力を待つだけでなく、状況に応じてAI側から話しかける自律性を備える。

1.2 インスピレーション

ねうろ様、shizuku AI、AIにけちゃん等のAI Vtuber。

2. システムアーキテクチャ

フロントエンド（Webブラウザ）を主軸とし、重い処理やPython依存の機能はAPI経由で呼び出すハイブリッド構成を採用する。

2.1 技術スタック

| カテゴリ | 採用技術 | 詳細・役割 |
| Frontend | Next.js (App Router) | アプリケーションの基盤、UI/UX管理 |
| Language | TypeScript | 型安全な開発の担保 |
| Styling | Tailwind CSS | モダンでクリーンなUIデザイン |
| 3D Engine | @react-three/fiber | ブラウザ上でのVRMモデル描画・制御 |
| LLM | GPT-4o-mini | 高速・安価な対話エンジン |
| Database | AstraDB | 会話履歴の保存、およびベクター検索 |
| TTS (音声) | ElevenLabs | 自然で表現豊かな音声合成（API経由） |
| Backend (推奨) | FastAPI (Python) | TTSおよび複雑なロジック用APIサーバー |

3. 機能設計

3.1 感情・親愛度ロジック（Internal State）

AIの内部状態を AI_State オブジェクトとして定義し、以下の要素を管理する。

感情（Emotion）:

LLMの応答から「喜び・怒り・悲しみ・楽しみ」を抽出。

反映: 3Dモデルの表情切り替え、音声のトーン変更（ElevenLabsのパラメータ）。

親愛度（Affinity）:

会話の積み重ねで上昇。

反映: System Prompt内の性格設定を動的に書き換え（例：「丁寧な口調」→「親密な口調」）。

3.2 自律発話（Autonomous Behavior）

WebSocket / SSE: サーバー側からのプッシュ通信。

ロジック: 最終発話からの経過時間や特定の時間帯をトリガーに、AI側から挨拶や問いかけを行う。

3.3 記憶管理（Memory）

AstraDBの活用:

短期記憶：直近の会話履歴（Context）をLLMへ送信。

長期記憶：過去の重要なエピソードをベクター検索で取り出し、プロンプトへ注入。

4. 実装ロードマップ

Phase 1: 基礎基盤（Current Focus）

$$x$$

 Next.jsプロジェクトのセットアップ

$$x$$

 GPT-4o-mini APIとの連携（基本チャット）

$$$$

 AstraDBとの接続設定

Phase 2: 肉体と声の付与

$$$$

 @react-three/fiber によるVRMモデル表示

$$$$

 ElevenLabs APIとの連携設定

$$$$

 音声再生と簡易リップシンク（口パク）の実装

Phase 3: 知能の高度化

$$$$

 感情分析エンジンの実装（LLM返答からのタグ抽出）

$$$$

 感情に応じた3D表情アニメーションの実装

$$$$

 親愛度変数の保存とプロンプトへの反映

Phase 4: 自律性の獲得

$$$$

 WebSocketによる双方向通信の確立

$$$$

 放置時間に応じた自律発話ロジックの実装

5. 開発上の重要メモ

パフォーマンス: TTSの生成待ち時間をどうUIでカバーするか（「考え中」モーションの導入など）。

拡張性: 今後、Webカメラを用いたユーザーの表情認識や、マイク入力による音声会話への拡張も視野に入れる。